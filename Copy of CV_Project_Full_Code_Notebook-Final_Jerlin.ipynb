{"cells":[{"cell_type":"markdown","metadata":{"id":"v9VXT5unCaBd"},"source":["# Introduction to Computer Vision: Plant Seedlings Classification"]},{"cell_type":"markdown","metadata":{"id":"4ZgcS1MyVGZp"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"WCxSmokWEKUJ"},"source":["### Context"]},{"cell_type":"markdown","metadata":{"id":"-2mC12JhVNGp"},"source":["In recent times, the field of agriculture has been in urgent need of modernizing, since the amount of manual work people need to put in to check if plants are growing correctly is still highly extensive. Despite several advances in agricultural technology, people working in the agricultural industry still need to have the ability to sort and recognize different plants and weeds, which takes a lot of time and effort in the long term. The potential is ripe for this trillion-dollar industry to be greatly impacted by technological innovations that cut down on the requirement for manual labor, and this is where Artificial Intelligence can actually benefit the workers in this field, as **the time and energy required to identify plant seedlings will be greatly shortened by the use of AI and Deep Learning.** The ability to do so far more efficiently and even more effectively than experienced manual labor, could lead to better crop yields, the freeing up of human inolvement for higher-order agricultural decision making, and in the long term will result in more sustainable environmental practices in agriculture as well.\n"]},{"cell_type":"markdown","metadata":{"id":"q_I9gQJMVWL_"},"source":["### Objective"]},{"cell_type":"markdown","metadata":{"id":"ZkD5j4o4VYYQ"},"source":["The aim of this project is to Build a Convolutional Neural Netowrk to classify plant seedlings into their respective categories."]},{"cell_type":"markdown","metadata":{"id":"aZq8uFtOVfnm"},"source":["### Data Dictionary"]},{"cell_type":"markdown","metadata":{"id":"75fTG3prVjUU"},"source":["The Aarhus University Signal Processing group, in collaboration with the University of Southern Denmark, has recently released a dataset containing **images of unique plants belonging to 12 different species.**\n","\n","- The dataset can be download from Olympus.\n","- The data file names are:\n","    - images.npy\n","    - Labels.csv\n","- Due to the large volume of data, the images were converted to the images.npy file and the labels are also put into Labels.csv, so that you can work on the data/project seamlessly without having to worry about the high data volume.\n","\n","- The goal of the project is to create a classifier capable of determining a plant's species from an image.\n","\n","**List of Species**\n","\n","- Black-grass\n","- Charlock\n","- Cleavers\n","- Common Chickweed\n","- Common Wheat\n","- Fat Hen\n","- Loose Silky-bent\n","- Maize\n","- Scentless Mayweed\n","- Shepherds Purse\n","- Small-flowered Cranesbill\n","- Sugar beet"]},{"cell_type":"markdown","metadata":{"id":"d9B4COveVqnm"},"source":["### **Note: Please use GPU runtime on Google Colab to execute the code faster.**"]},{"cell_type":"markdown","metadata":{"id":"qqFzmTb0BKKW"},"source":["## Importing necessary libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":90800,"status":"ok","timestamp":1734812117553,"user":{"displayName":"Jerlin John","userId":"05662701987174028885"},"user_tz":360},"id":"JAkYVkhPb0p7","outputId":"a1fbbf2b-20dd-49a7-a398-adede1ba239c"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.10 are installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.10.1 requires pandas\u003c2.2.3dev0,\u003e=2.0, but you have pandas 1.5.3 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n","mizani 0.13.1 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","mlxtend 0.23.3 requires scikit-learn\u003e=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","plotnine 0.14.4 requires matplotlib\u003e=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n","plotnine 0.14.4 requires pandas\u003e=2.2.0, but you have pandas 1.5.3 which is incompatible.\n","tensorstore 0.1.71 requires ml_dtypes\u003e=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n","tf-keras 2.17.0 requires tensorflow\u003c2.18,\u003e=2.17, but you have tensorflow 2.15.0 which is incompatible.\n","xarray 2024.11.0 requires pandas\u003e=2.1, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# Installing the libraries with the specified version.\n","# uncomment and run the following line if Google Colab is being used\n","!pip install tensorflow==2.15.0 scikit-learn==1.2.2 seaborn==0.13.1 matplotlib==3.7.1 numpy==1.25.2 pandas==1.5.3 opencv-python==4.8.0.76 -q --user"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":183582,"status":"ok","timestamp":1734812301132,"user":{"displayName":"Jerlin John","userId":"05662701987174028885"},"user_tz":360},"id":"lw8IuZwV-PAL"},"outputs":[],"source":["# Installing the libraries with the specified version.\n","# uncomment and run the following lines if Jupyter Notebook is being used\n","!pip install tensorflow==2.13.0 scikit-learn==1.2.2 seaborn==0.11.1 matplotlib==3.3.4 numpy==1.24.3 pandas==1.5.2 opencv-python==4.8.0.76 -q --user"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":21101,"status":"ok","timestamp":1734812322229,"user":{"displayName":"Jerlin John","userId":"05662701987174028885"},"user_tz":360},"id":"QL8oq6tetNib"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import cv2\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import classification_report, confusion_matrix\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from google.colab.patches import cv2_imshow\n","\n","#ignore warnings\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"NbFXvdwks2iR"},"source":["**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the start again.*"]},{"cell_type":"markdown","metadata":{"id":"oq-hAEOAV4aZ"},"source":["## Loading the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1XztFCIActAb"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-4-7938ddeefcea\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Uncomment and run the below code if you are using google colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--\u003e 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["# Uncomment and run the below code if you are using google colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c2q2QUVZtpFb"},"outputs":[],"source":["#Load the image file of the dataset\n","import numpy as np\n","import pandas as pd\n","images = np.load('/content/drive/MyDrive/Introduction to Computer Vision: Plant Seedlings Classification project8/images.npy')\n","\n","#Load the labels file of the dataset\n","labels = pd.read_csv('/content/drive/MyDrive/Introduction to Computer Vision: Plant Seedlings Classification project8/Labels.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"uE84hQU7CSZa"},"source":["## Data Overview"]},{"cell_type":"markdown","metadata":{"id":"57vwo75fcXbU"},"source":["### Understand the shape of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"S2F57JGGcbzz"},"outputs":[],"source":["print (images.shape)"]},{"cell_type":"markdown","metadata":{"id":"yeqPxWJaw8LP"},"source":["Observation:\n","There are 4750 RGB images of shape 128x128x3 ,each image having three channels."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y1prmR5ewnbu"},"outputs":[],"source":["print (labels.shape)"]},{"cell_type":"markdown","metadata":{"id":"VJwj9HAxy5oj"},"source":["Plotting images using Open CV and matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"N4WAWaILzE_q"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","cv2_imshow(images[5])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iG9pe5-YzZcy"},"outputs":[],"source":["plt.imshow(images[5])"]},{"cell_type":"markdown","metadata":{"id":"EYv5uX-MC9KC"},"source":["## Exploratory Data Analysis"]},{"cell_type":"markdown","metadata":{"id":"Vf_sWYNOjDvK"},"source":["- EDA is an important part of any project involving data.\n","- It is important to investigate and understand the data better before building a model with it.\n","- A few questions have been mentioned below which will help you understand the data better.\n","- A thorough analysis of the data, in addition to the questions mentioned below, should be done."]},{"cell_type":"markdown","metadata":{"id":"F4dZsdgujrtK"},"source":["1. How are these different category plant images different from each other?\n","2. Is the dataset provided an imbalance? (Check with using bar plots)"]},{"cell_type":"markdown","metadata":{"id":"vOvazq-OWpNB"},"source":["## Data Pre-Processing"]},{"cell_type":"markdown","metadata":{"id":"hzpcKHaDWsG7"},"source":["### Convert the BGR images to RGB images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"U9u82V2TWsuQ"},"outputs":[],"source":["for i in range(len(images)):\n","    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FWgqfGbNzziE"},"outputs":[],"source":["def plot_image(image):\n","    plt.figure(figsize = (12,12))\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()\n","\n","plot_image(images[5])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Qiq2V5B33l8u"},"outputs":[],"source":["def plot_images(images,labels):\n","  num_classes = 10\n","  categories=np.unique(labels)\n","  fig, axes = plt.subplots(1, num_classes, figsize=(20, 20))\n","  keys = dict(labels['Label'])\n","  rows = 3\n","  cols = 4\n","  for i in range(10):\n","    keys[i] = categories\n","    indices = np.where(labels == categories[i])\n","    indices = np.arange(indices[0].shape[0])\n","    np.random.shuffle(indices)\n","    axes[i].imshow(images[indices[0]])\n","    axes[i].set_title(categories[i])\n","    axes[i].axis('off')\n","  plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Oqh26M4B35IE"},"outputs":[],"source":["plot_images(images,labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ru4o3gzy4ZR4"},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","sns.set_style('darkgrid')\n","\n","sns.countplot(x=labels['Label'])\n","plt.title('Count of each category')\n","plt.xticks(rotation=90)\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fca6929_5_db"},"source":["Observation:\n","Dataset is quite balanced.\n","Loose Silky-bent have large count (~650) out of 12 plant species."]},{"cell_type":"markdown","metadata":{"id":"STMonBqiWxM5"},"source":["### Resize the images"]},{"cell_type":"markdown","metadata":{"id":"pESDU0AEMOFk"},"source":["As the size of the images is large, it may be computationally expensive to train on these larger images; therefore, it is preferable to reduce the image size from 128 to 64."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kJYZ4IpGkwre"},"outputs":[],"source":["images = np.array([cv2.resize(img, (64, 64)) for img in images])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"23SRgCXs6mPL"},"outputs":[],"source":["images_decreased=[]\n","for i in range(len(images)):\n","    images_decreased.append(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))\n","    height = images[i].shape[0]\n","    width = images[i].shape[1]\n","    dim = (width, height)\n","    resized = cv2.resize(images[i], dim, interpolation = cv2.INTER_AREA)\n","    images_decreased.append(resized)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QAiuYDr664au"},"outputs":[],"source":["plt.imshow(images_decreased[5])"]},{"cell_type":"markdown","metadata":{"id":"cLUaXQty7GvW"},"source":["Visualizing images using Gaussian Blur"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ohn1m5zX7F2H"},"outputs":[],"source":["#Applying Gaussian Blur to denoise the images\n","images_gb = np.array([cv2.GaussianBlur(img, (5, 5), 0) for img in images])\n","for i in range(len(images_gb)):\n","    images_gb[i] = cv2.cvtColor(images_gb[i], cv2.COLOR_BGR2RGB)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"n-jg8XPi7lXR"},"outputs":[],"source":["plt.imshow(images_gb[5])"]},{"cell_type":"markdown","metadata":{"id":"rCLfGXD07ugc"},"source":["It appears that Gaussian Blurr image would be ineffective because the blurred or denoised image does not seems to contain any relevant information,and the model would struggle to categorise these blurred images."]},{"cell_type":"markdown","metadata":{"id":"LdsVQb4umB0P"},"source":["### Data Preparation for Modeling"]},{"cell_type":"markdown","metadata":{"id":"KljdsjFCmJIZ"},"source":["- Before you proceed to build a model, you need to split the data into train, test, and validation to be able to evaluate the model that you build on the train data\n","- You'll have to encode categorical features and scale the pixel values.\n","- You will build a model using the train data and then check its performance"]},{"cell_type":"markdown","metadata":{"id":"NQV0unTvM7XM"},"source":["**Split the dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1UB1kXpm8jRC"},"outputs":[],"source":["#Splitting the dataset\n","from sklearn.model_selection import train_test_split\n","X_temp, X_test, y_temp, y_test = train_test_split(images,labels , test_size=0.1, random_state=42,stratify=labels)\n","X_train, X_val, y_train, y_val = train_test_split(X_temp,y_temp , test_size=0.1, random_state=42,stratify=y_temp)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZSR1u-Dv9FYW"},"outputs":[],"source":["print(X_train.shape,y_train.shape)\n","print(X_val.shape,y_val.shape)\n","print(X_test.shape,y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mKZRBzJJ-PDf"},"outputs":[],"source":["y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UXq2gi0690Af"},"outputs":[],"source":["y_train_encoded_shape = y_train.shape\n","y_test_encoded_shape = y_test.shape\n","y_val_encoded_shape = y_val.shape\n","print(y_train_encoded_shape)\n","print(y_test_encoded_shape)\n","print(y_val_encoded_shape)"]},{"cell_type":"markdown","metadata":{"id":"exJFCDSMNrEG"},"source":["### Data Normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jVUuPJS9OB_U"},"outputs":[],"source":["X_train_normalized = X_train.astype('float32')/255.0\n","X_val_normalized = X_val.astype('float32')/255.0\n","X_test_normalized = X_test.astype('float32')/255.0"]},{"cell_type":"markdown","metadata":{"id":"N7IVMl2umv_x"},"source":["# Encoding the target labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7_QVm-2zmvwM"},"outputs":[],"source":["# Convert labels from names to one hot vectors.\n","# We have already used encoding methods like onehotencoder and labelencoder earlier so now we will be using a new encoding method called labelBinarizer.\n","# Labelbinarizer works similar to onehotencoder\n","\n","from sklearn.preprocessing import LabelBinarizer\n","enc = LabelBinarizer()\n","y_train_encoded = enc.fit_transform(y_train)\n","y_val_encoded=enc.transform(y_val)\n","y_test_encoded=enc.transform(y_test)"]},{"cell_type":"markdown","metadata":{"id":"d9_M19L-OLng"},"source":["## Model Building"]},{"cell_type":"markdown","metadata":{"id":"eq62wBakjIvF"},"source":["# CNN - Convolutional Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w0fnV8yNKmYr"},"outputs":[],"source":["#clearing backend\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","tf.keras.backend.clear_session()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-fpQ3dVP_EWZ"},"outputs":[],"source":["#Fixing te seed for random number generators\n","import random\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tNWQgSkazVAY"},"outputs":[],"source":["model = Sequential()\n","\n","\n","# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n","# Input_shape denotes input image dimension of images\n","model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model.add(MaxPooling2D((2, 2), padding = 'same'))\n","\n","model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n","model.add(MaxPooling2D((2, 2), padding = 'same'))\n","\n","# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n","model.add(Flatten())\n","\n","# Adding a fully connected dense layer with 100 neurons\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.3))\n","# Adding the output layer with 12 neurons and activation functions as softmax since this is a multi-class classification problem\n","model.add(Dense(12, activation='softmax'))\n","\n","# Using SGD Optimizer\n","# opt = SGD(learning_rate=0.01, momentum=0.9)\n","opt=Adam()\n","# Compile model\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"xDmwaW_XCAiM"},"source":["# Fitting the model on the train data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FAv6VMHDqMnD"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","61/61 - 274s - loss: 2.4381 - accuracy: 0.1328 - val_loss: 2.2665 - val_accuracy: 0.2617 - 274s/epoch - 4s/step\n","Epoch 2/30\n","61/61 - 242s - loss: 2.2251 - accuracy: 0.2667 - val_loss: 2.0530 - val_accuracy: 0.3458 - 242s/epoch - 4s/step\n","Epoch 3/30\n","61/61 - 237s - loss: 2.1072 - accuracy: 0.2839 - val_loss: 1.9908 - val_accuracy: 0.3154 - 237s/epoch - 4s/step\n","Epoch 4/30\n","61/61 - 247s - loss: 2.0234 - accuracy: 0.2904 - val_loss: 1.8276 - val_accuracy: 0.3715 - 247s/epoch - 4s/step\n","Epoch 5/30\n","61/61 - 239s - loss: 1.9417 - accuracy: 0.3161 - val_loss: 1.7360 - val_accuracy: 0.4416 - 239s/epoch - 4s/step\n","Epoch 6/30\n","61/61 - 245s - loss: 1.8850 - accuracy: 0.3296 - val_loss: 1.6218 - val_accuracy: 0.4369 - 245s/epoch - 4s/step\n","Epoch 7/30\n","61/61 - 242s - loss: 1.7920 - accuracy: 0.3546 - val_loss: 1.5789 - val_accuracy: 0.4930 - 242s/epoch - 4s/step\n","Epoch 8/30\n","61/61 - 240s - loss: 1.7245 - accuracy: 0.3759 - val_loss: 1.4548 - val_accuracy: 0.5561 - 240s/epoch - 4s/step\n","Epoch 9/30\n","61/61 - 245s - loss: 1.6423 - accuracy: 0.3930 - val_loss: 1.3695 - val_accuracy: 0.5537 - 245s/epoch - 4s/step\n","Epoch 10/30\n","61/61 - 241s - loss: 1.6021 - accuracy: 0.4084 - val_loss: 1.3534 - val_accuracy: 0.5164 - 241s/epoch - 4s/step\n","Epoch 11/30\n","61/61 - 247s - loss: 1.5892 - accuracy: 0.3982 - val_loss: 1.3554 - val_accuracy: 0.5724 - 247s/epoch - 4s/step\n","Epoch 12/30\n","61/61 - 239s - loss: 1.5066 - accuracy: 0.4401 - val_loss: 1.3152 - val_accuracy: 0.5631 - 239s/epoch - 4s/step\n","Epoch 13/30\n","61/61 - 237s - loss: 1.4440 - accuracy: 0.4487 - val_loss: 1.2538 - val_accuracy: 0.6051 - 237s/epoch - 4s/step\n","Epoch 14/30\n","61/61 - 241s - loss: 1.3733 - accuracy: 0.4822 - val_loss: 1.1820 - val_accuracy: 0.6379 - 241s/epoch - 4s/step\n","Epoch 15/30\n"]}],"source":["#Fitting the model on the train data\n","num_epochs = 30\n","batch_size = 64\n","history_1 = model.fit(X_train_normalized,\n","                    y_train_encoded,\n","                    epochs=num_epochs,\n","                    validation_data=(X_val_normalized,y_val_encoded),\n","                    batch_size=batch_size,\n","                    verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"4it8wQGWCTzf"},"source":["Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UrgV69qMCcj7"},"outputs":[],"source":["plt.plot(history_1.history['accuracy'])\n","plt.plot(history_1.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Falz2h2SCskM"},"source":["Observation:\n","Based on validation accuracy, this model is underfitting."]},{"cell_type":"markdown","metadata":{"id":"3F5YCwTNDYnO"},"source":["Evaluating the model on test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KbdJrsi-DyuA"},"outputs":[],"source":["accuracy = model.evaluate(X_test_normalized, y_test_encoded, verbose=2)\n","print('Test accuracy:', accuracy[1])\n"]},{"cell_type":"markdown","metadata":{"id":"wGhh7mNy4Rh_"},"source":["Test accuracy shows this model is underfitting."]},{"cell_type":"markdown","metadata":{"id":"-TgaNHCIEBFf"},"source":["Generating the predictions using test data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"WwULQYthDhf9"},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=model.predict(X_test_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"FzplSPneEFRu"},"outputs":[],"source":["y_pred"]},{"cell_type":"markdown","metadata":{"id":"XTrFjyz1EKPb"},"source":["Plotting the confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nO8LV00PERLN"},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(10, 8))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XUugLeerFlwT"},"source":["Observations:\n","\n","* Some of the classes are not predicted correctly.\n","* In comparison to the rest, we can see that 3 and 6 classes are well classified.\n","* Also, it is remarkable that 0,1,2,4,5,7,8,9 are mostly miclassified.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Qog0E8WHzrva"},"source":["# **Model**-2"]},{"cell_type":"markdown","metadata":{"id":"LQXu8fDo2aQN"},"source":["As we noticed that my initial model appears to overfit. Therefore, I am trying to address this problem with data augmentation and Batch Normalization to check if i can improve the model's performance."]},{"cell_type":"markdown","metadata":{"id":"kNKUalx8Jcoi"},"source":["## Model Performance Improvement"]},{"cell_type":"markdown","metadata":{"id":"s_oS4D_AXFqX"},"source":["**Reducing the Learning Rate:**\n","\n","**Hint**: Use **ReduceLRonPlateau()** function that will be used to decrease the learning rate by some factor, if the loss is not decreasing for some time. This may start decreasing the loss at a smaller learning rate. There is a possibility that the loss may still not decrease. This may lead to executing the learning rate reduction again in an attempt to achieve a lower loss."]},{"cell_type":"markdown","metadata":{"id":"zU6vqL67bd5a"},"source":["### **Data Augmentation**\n","\n","Remember, **data augmentation should not be used in the validation/test data set**."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i-pngeq23SUb"},"outputs":[],"source":["#Clearing backend\n","from tensorflow.keras import backend as K\n","tf.keras.backend.clear_session()\n","\n","#Fixing the seed for random number generators\n","import random\n","random.seed(42)\n","np.random.seed(42)\n","tf.random.set_seed(42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MOVEWXy03bxZ"},"outputs":[],"source":["# All images to be rescaled by 1/255.\n","train_datagen = ImageDataGenerator(\n","                              rotation_range=20,\n","                              fill_mode='nearest'\n","                              )\n","# test_datagen  = ImageDataGenerator(rescale = 1.0/255.)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wjQUe2go0XhX"},"outputs":[],"source":["# Intializing a sequential model\n","model = Sequential()\n","\n","# Adding first conv layer with 64 filters and kernel size 3x3 , padding 'same' provides the output size same as the input size\n","# Input_shape denotes input image dimension images\n","model.add(Conv2D(128, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model.add(Conv2D(64, (3, 3), activation='relu', padding=\"same\", input_shape=(64, 64, 3)))\n","\n","# Adding max pooling to reduce the size of output of first conv layer\n","model.add(MaxPooling2D((2, 2), padding = 'same'))\n","# model.add(BatchNormalization())\n","model.add(Conv2D(32, (3, 3), activation='relu', padding=\"same\"))\n","model.add(MaxPooling2D((2, 2), padding = 'same'))\n","from tensorflow.keras.layers import BatchNormalization\n","model.add(BatchNormalization())\n","# flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n","model.add(Flatten())\n","\n","# Adding a fully connected dense layer with 100 neurons\n","model.add(Dense(16, activation='relu'))\n","model.add(Dropout(0.3))\n","# Adding the output layer with 12 neurons and activation functions as softmax since this is a multi-class classification problem\n","model.add(Dense(12, activation='softmax'))\n","\n","# Using SGD Optimizer\n","# opt = SGD(learning_rate=0.01, momentum=0.9)\n","opt=Adam()\n","# Compile model\n","model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"r0ggnfTDyn4E"},"outputs":[],"source":["# Epochs\n","epochs = 30\n","# Batch size\n","batch_size = 64\n","num_classes = 12\n","history= model.fit(train_datagen.flow(X_train_normalized,y_train_encoded,\n","                                       batch_size=batch_size,\n","                                       seed=42,\n","                                       shuffle=False),\n","                    epochs=epochs,\n","                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n","                    validation_data=(X_val_normalized,y_val_encoded),\n","                    verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"k-J2OhiirNIA"},"source":["This model is also underfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"47p3NbKK95uY"},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0ewAhmr8PYYq"},"source":["#Observation\n","1. The Train accuracy and Validation accuracy over a number of epochs.\n","\n","2. The validation accuracy appears to be around 67%, indicating that the model may not be performing well on validation data.\n","3. There are fluctuations in both training and validation accuracies, which could suggest underfitting or instability in the training process."]},{"cell_type":"markdown","metadata":{"id":"BE55Eu3wzgwJ"},"source":["Observation: Validation accuracy rate is very low. This is definetly not a good model to select for this problem."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qT7q9IBe-FEb"},"outputs":[],"source":["accuracy = model.evaluate(X_test_normalized, y_test_encoded, verbose=2)\n","print('Test accuracy:', accuracy[1])\n"]},{"cell_type":"markdown","metadata":{"id":"mlY7kvgsz2HS"},"source":["Observation: This model is underfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7c1-x4KP-L91"},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=model.predict(X_test_normalized)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cy8kahizMEkN"},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(10, 8))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"V0IkNSy0-n9f"},"source":["# Model - 3 Transfer Learning using VGG16"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KXskYBhgrTG8"},"outputs":[],"source":["from tensorflow.keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","\n","vgg_model = VGG16(weights='imagenet', include_top = False, input_shape = (64,64,3))\n","vgg_model.summary()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cWZGocOB75CO"},"outputs":[],"source":["#Making all the layers of the VGG model non-trainable\n","for layer in vgg_model.layers:\n","    layer.trainable = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-ggkWaaFrYuE"},"outputs":[],"source":["new_model = Sequential()\n","\n","# Adding the convolutional part of the VGG16 model from above\n","new_model.add(vgg_model)\n","\n","# Flattening the output of the VGG16 model because it is from a convolutional layer\n","new_model.add(Flatten())\n","\n","# Adding a dense output layer\n","new_model.add(Dense(1024, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(512, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(256, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(128, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(64, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(32, activation='relu'))\n","new_model.add(Dropout(0.2))\n","new_model.add(Dense(16, activation='relu'))\n","new_model.add(Dense(12, activation='softmax'))\n","opt=Adam()\n","# Compile model\n","new_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Generating the summary of the model\n","new_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"stq1fw9hrgYP"},"outputs":[],"source":["# Epochs\n","epochs = 30\n","# Batch size\n","batch_size = 64\n","\n","history_vgg16 = new_model.fit(train_datagen.flow(X_train_normalized,y_train_encoded,\n","                                       batch_size=batch_size,\n","                                       seed=42,\n","                                       shuffle=False),\n","                    epochs=epochs,\n","                    steps_per_epoch=X_train_normalized.shape[0] // batch_size,\n","                    validation_data=(X_val_normalized,y_val_encoded),\n","                    verbose=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NL6ou-2VrmT2"},"outputs":[],"source":["plt.plot(history_vgg16.history['accuracy'])\n","plt.plot(history_vgg16.history['val_accuracy'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"qjsRV1hSuRIU"},"source":["Observation:\n","Validation accuracy shows better than train accuracy in VGG16 model."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"U0k7RC4Frs25"},"outputs":[],"source":["accuracy = new_model.evaluate(X_test_normalized, y_test_encoded, verbose=2)"]},{"cell_type":"markdown","metadata":{"id":"2SRyPCmv2Yqi"},"source":["This model is also underfitting which is only 49% test accuracy."]},{"cell_type":"markdown","metadata":{"id":"B9QXHO_0w2Bu"},"source":["Test accuracy shows underfitting as well but this model is better than other models."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"pnlFDRd-rwWk"},"outputs":[],"source":["# Here we would get the output as probablities for each category\n","y_pred=new_model.predict(X_test_normalized)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ryt1sZLmr1qM"},"outputs":[],"source":["# Obtaining the categorical values from y_test_encoded and y_pred\n","y_pred_arg=np.argmax(y_pred,axis=1)\n","y_test_arg=np.argmax(y_test_encoded,axis=1)\n","\n","# Plotting the Confusion Matrix using confusion matrix() function which is also predefined tensorflow module\n","confusion_matrix = tf.math.confusion_matrix(y_test_arg,y_pred_arg)\n","f, ax = plt.subplots(figsize=(10, 8))\n","sns.heatmap(\n","    confusion_matrix,\n","    annot=True,\n","    linewidths=.4,\n","    fmt=\"d\",\n","    square=True,\n","    ax=ax\n",")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"h7BUovkZKTLJ"},"source":["According to confusion matrix and accurcy curve the VGG16 model has outperformced Model-2."]},{"cell_type":"markdown","metadata":{"id":"A5cgkAz_YKcc"},"source":["## Final Model"]},{"cell_type":"markdown","metadata":{"id":"F3MjkGfHYOPn"},"source":["Comment on the final model you have selected and use the same in the below code to visualize the image."]},{"cell_type":"markdown","metadata":{"id":"v3yhJGLFKx9_"},"source":["Based on three models, CNN Model with Data Augmentation performed better than others.Therefore, I am selecting model_2 is the best model for this problem. I am using Data Augmentation to predict and visualize some test images."]},{"cell_type":"markdown","metadata":{"id":"dvDkLMO7YIdY"},"source":["### Visualizing the prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xCJJ4vposL9Q"},"outputs":[],"source":["# Visualizing the predicted and correct label of images from test data\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[2])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(new_model.predict((X_test_normalized[2].reshape(1,64,64,3)))))   # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[2])                                               # using inverse_transform() to get the output label from the output vector\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[33])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(new_model.predict((X_test_normalized[33].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[33])                                              # using inverse_transform() to get the output label from the output vector\n","\n","plt.figure(figsize=(2,2))\n","plt.imshow(X_test[36])\n","plt.show()\n","print('Predicted Label', enc.inverse_transform(new_model.predict((X_test_normalized[36].reshape(1,64,64,3)))))  # reshaping the input image as we are only trying to predict using a single image\n","print('True Label', enc.inverse_transform(y_test_encoded)[36])"]},{"cell_type":"markdown","metadata":{"id":"Eg2x8AyJ4oPR"},"source":["# Actionable Insights and Business Recommendations"]},{"cell_type":"markdown","metadata":{"id":"5O_NerrdsVZ7"},"source":["##Conclusion/Actionable Insights:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tYmkwcACsze_"},"outputs":[],"source":["pd.DataFrame({'Models': ['Base CNN Model', 'CNN Model with Data Augmentation', 'Transfer Learning Model'],\n","              'Train Accuracy': [history_1.history['accuracy'][-1], history.history['accuracy'][-1], history_vgg16.history['accuracy'][-1]],\n","              'Validation Accuracy': [history_1.history['val_accuracy'][-1], history.history['val_accuracy'][-1], history_vgg16.history['val_accuracy'][-1]]})"]},{"cell_type":"markdown","metadata":{"id":"cbWwOBkJ2qpF"},"source":["Based on above comparison, CNN model with data augmentation is the best model since it has almost 68% validation accuracy."]},{"cell_type":"markdown","metadata":{"id":"jvq4_6CZYcrv"},"source":["# Scope of Improvement/Business Recommnendations\n","\n","* These models can be further optimized by training with different filter sizes and varying numbers of filters.\n","*  These models can also be trained on the original image size, i.e., 128 x 128, rather than being reduced to 64.\n","* More extensive data augmentation can be performed, and the dropout rate can be modified to improve model performance. Other transfer learning architectures can also be leveraged to train the CNN model, and these models can be utilized for classification."]},{"cell_type":"markdown","metadata":{"id":"geC4LwwIYfS_"},"source":["_____"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["Eg2x8AyJ4oPR"],"name":"","provenance":[{"file_id":"1Zzu7SYmj7DSv--39t_JI4GC61bM3qfC6","timestamp":1734761894524}],"version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}